{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.12/2.4.3/spark-sql_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/sh/almond/almond-spark_2.12/0.9.1/almond-spark_2.12-0.9.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.12/2.4.3/spark-sql_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/sh/almond/almond-spark_2.12/0.9.1/almond-spark_2.12-0.9.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-parent_2.12/2.4.3/spark-parent_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-parent_2.12/2.4.3/spark-parent_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/apache/18/apache-18.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/apache/18/apache-18.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-catalyst_2.12/2.4.3/spark-catalyst_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sketch_2.12/2.4.3/spark-sketch_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/orc/orc-mapreduce/1.5.5/orc-mapreduce-1.5.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.8/scala-library-2.12.8.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.8/scala-library-2.12.8.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-sketch_2.12/2.4.3/spark-sketch_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/univocity/univocity-parsers/2.7.3/univocity-parsers-2.7.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/parquet/parquet-column/1.10.1/parquet-column-1.10.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/sh/almond/ammonite-spark_2.12/0.7.2/ammonite-spark_2.12-0.7.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/xbean/xbean-asm6-shaded/4.8/xbean-asm6-shaded-4.8.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/orc/orc-core/1.5.5/orc-core-1.5.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.3/spark-core_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/parquet/parquet-hadoop/1.10.1/parquet-hadoop-1.10.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-catalyst_2.12/2.4.3/spark-catalyst_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-tags_2.12/2.4.3/spark-tags_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-tags_2.12/2.4.3/spark-tags_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.7.1/jackson-databind-2.6.7.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/arrow/arrow-vector/0.10.0/arrow-vector-0.10.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/sh/almond/ammonite-spark_2.12/0.7.2/ammonite-spark_2.12-0.7.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/2.4.3/spark-core_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/xbean/xbean/4.8/xbean-4.8.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/orc/orc/1.5.5/orc-1.5.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/parquet/parquet/1.10.1/parquet-1.10.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/arrow/arrow-java-root/0.10.0/arrow-java-root-0.10.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/jackson-parent/2.6.2/jackson-parent-2.6.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/arrow/arrow-java-root/0.10.0/arrow-java-root-0.10.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/jackson-parent/2.6.2/jackson-parent-2.6.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/xbean/xbean/4.8/xbean-4.8.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/orc/orc/1.5.5/orc-1.5.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/parquet/parquet/1.10.1/parquet-1.10.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/geronimo/genesis/genesis-java5-flava/2.1/genesis-java5-flava-2.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/apache/16/apache-16.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/oss-parent/24/oss-parent-24.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/geronimo/genesis/genesis-java5-flava/2.1/genesis-java5-flava-2.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/oss-parent/24/oss-parent-24.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/apache/16/apache-16.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/geronimo/genesis/genesis-default-flava/2.1/genesis-default-flava-2.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/geronimo/genesis/genesis-default-flava/2.1/genesis-default-flava-2.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/geronimo/genesis/genesis/2.1/genesis-2.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/geronimo/genesis/genesis/2.1/genesis-2.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/apache/13/apache-13.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/apache/13/apache-13.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/commons-net/commons-net/3.1/commons-net-3.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/vlkan/flatbuffers/1.2.0-3f79e055/flatbuffers-1.2.0-3f79e055.pom\n",
      "Downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/twitter/chill-java/0.9.3/chill-java-0.9.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/commons-net/commons-net/3.1/commons-net-3.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/net/razorvine/pyrolite/4.13/pyrolite-4.13.pom\n",
      "Downloading https://repo1.maven.org/maven2/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/twitter/chill-java/0.9.3/chill-java-0.9.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/3.1.5/metrics-graphite-3.1.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.12/1.1.0/scala-parser-combinators_2.12-1.1.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-network-shuffle_2.12/2.4.3/spark-network-shuffle_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-crypto/1.0.0/commons-crypto-1.0.0.pom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.12/1.1.0/scala-parser-combinators_2.12-1.1.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/codehaus/janino/janino/3.0.9/janino-3.0.9.pom\n",
      "Downloaded https://repo1.maven.org/maven2/net/razorvine/pyrolite/4.13/pyrolite-4.13.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.8.2/avro-1.8.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-json/3.1.5/metrics-json-3.1.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/net/sf/py4j/py4j/0.10.7/py4j-0.10.7.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/codehaus/janino/janino/3.0.9/janino-3.0.9.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-network-shuffle_2.12/2.4.3/spark-network-shuffle_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/jcl-over-slf4j/1.7.16/jcl-over-slf4j-1.7.16.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet/2.22.2/jersey-container-servlet-2.22.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.4.20.v20190813/jetty-server-9.4.20.v20190813.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/avro/avro/1.8.2/avro-1.8.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/parquet/parquet-jackson/1.10.1/parquet-jackson-1.10.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.4.20.v20190813/jetty-server-9.4.20.v20190813.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/arrow/arrow-memory/0.10.0/arrow-memory-0.10.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/2.4.3/spark-unsafe_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-launcher_2.12/2.4.3/spark-launcher_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/carrotsearch/hppc/0.7.2/hppc-0.7.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/netty/netty/3.9.9.Final/netty-3.9.9.Final.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/parquet/parquet-common/1.10.1/parquet-common-1.10.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-unsafe_2.12/2.4.3/spark-unsafe_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/roaringbitmap/RoaringBitmap/0.7.45/RoaringBitmap-0.7.45.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-launcher_2.12/2.4.3/spark-launcher_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/parquet/parquet-encoding/1.10.1/parquet-encoding-1.10.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/parquet/parquet-format/2.4.0/parquet-format-2.4.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.pom\n",
      "Downloading https://repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/glassfish/jersey/containers/jersey-container-servlet-core/2.22.2/jersey-container-servlet-core-2.22.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.5/commons-lang3-3.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/twitter/chill_2.12/0.9.3/chill_2.12-0.9.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.9.9/joda-time-2.9.9.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/arrow/arrow-format/0.10.0/arrow-format-0.10.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.pom\n",
      "Downloading https://repo1.maven.org/maven2/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.pom\n",
      "Downloaded https://repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-network-common_2.12/2.4.3/spark-network-common_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-client/2.22.2/jersey-client-2.22.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/jul-to-slf4j/1.7.16/jul-to-slf4j-1.7.16.pom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/oro/oro/2.0.8/oro-2.0.8.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-network-common_2.12/2.4.3/spark-network-common_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.6.7.1/jackson-module-scala_2.12-2.6.7.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/codehaus/janino/commons-compiler/3.0.9/commons-compiler-3.0.9.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/oro/oro/2.0.8/oro-2.0.8.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/2.6.5/hadoop-client-2.6.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/netty/netty-all/4.1.17.Final/netty-all-4.1.17.Final.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/json4s/json4s-jackson_2.12/3.5.3/json4s-jackson_2.12-3.5.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.7.9/jackson-core-2.7.9.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/joda-time/joda-time/2.9.9/joda-time-2.9.9.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-server/2.22.2/jersey-server-2.22.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/glassfish/jersey/core/jersey-common/2.22.2/jersey-common-2.22.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.6.0/curator-recipes-2.6.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.7.9/jackson-core-2.7.9.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.3.2-2/zstd-jni-1.3.2-2.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/spark/spark-kvstore_2.12/2.4.3/spark-kvstore_2.12-2.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/3.1.5/metrics-core-3.1.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/spark/spark-kvstore_2.12/2.4.3/spark-kvstore_2.12-2.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/orc/orc-shims/1.5.5/orc-shims-1.5.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.10/commons-codec-1.10.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/lz4/lz4-java/1.4.0/lz4-java-1.4.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.pom\n",
      "Downloaded https://repo1.maven.org/maven2/commons-codec/commons-codec/1.10/commons-codec-1.10.pom\n",
      "Downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.pom\n",
      "Downloading https://repo1.maven.org/maven2/io/airlift/aircompressor/0.10/aircompressor-0.10.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-jvm/3.1.5/metrics-jvm-3.1.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/airlift/aircompressor/0.10/aircompressor-0.10.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-scala_2.12/2.6.7.1/jackson-module-scala_2.12-2.6.7.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/roaringbitmap/RoaringBitmapParent/0.7.45/RoaringBitmapParent-0.7.45.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-project-dist/2.6.5/hadoop-project-dist-2.6.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/avro/avro-parent/1.8.2/avro-parent-1.8.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/antlr/antlr4-master/4.7/antlr4-master-4.7.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/jackson-parent/2.7/jackson-parent-2.7.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/curator/apache-curator/2.6.0/apache-curator-2.6.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/roaringbitmap/RoaringBitmapParent/0.7.45/RoaringBitmapParent-0.7.45.pom\n",
      "Downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-parent/3.1.5/metrics-parent-3.1.5.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/jackson-parent/2.7/jackson-parent-2.7.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/glassfish/jersey/containers/project/2.22.2/project-2.22.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-project-dist/2.6.5/hadoop-project-dist-2.6.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/apache/7/apache-7.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/avro/avro-parent/1.8.2/avro-parent-1.8.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/codehaus/janino/janino-parent/3.0.9/janino-parent-3.0.9.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/curator/apache-curator/2.6.0/apache-curator-2.6.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/net/java/jvnet-parent/3/jvnet-parent-3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/glassfish/jersey/containers/project/2.22.2/project-2.22.2.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/41/commons-parent-41.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-parent/3.1.5/metrics-parent-3.1.5.pom\n",
      "Downloading https://repo1.maven.org/maven2/io/airlift/airbase/78/airbase-78.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/codehaus/janino/janino-parent/3.0.9/janino-parent-3.0.9.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/glassfish/jersey/project/2.22.2/project-2.22.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/apache/7/apache-7.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/carrotsearch/hppc-parent/0.7.2/hppc-parent-0.7.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/antlr/antlr4-master/4.7/antlr4-master-4.7.pom\n",
      "Downloading https://repo1.maven.org/maven2/io/netty/netty-parent/4.1.17.Final/netty-parent-4.1.17.Final.pom\n",
      "Downloaded https://repo1.maven.org/maven2/net/java/jvnet-parent/3/jvnet-parent-3.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/google/google/1/google-1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/google/google/1/google-1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.25/slf4j-parent-1.7.25.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/airlift/airbase/78/airbase-78.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-project/9.4.20.v20190813/jetty-project-9.4.20.v20190813.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/carrotsearch/hppc-parent/0.7.2/hppc-parent-0.7.2.pom\n",
      "Downloaded https://repo1.maven.org/maven2/io/netty/netty-parent/4.1.17.Final/netty-parent-4.1.17.Final.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.25/slf4j-parent-1.7.25.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-project/9.4.20.v20190813/jetty-project-9.4.20.v20190813.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/glassfish/jersey/project/2.22.2/project-2.22.2.pom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-parent/41/commons-parent-41.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/40/commons-parent-40.pom\n",
      "Downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/jackson-parent/2.6.1/jackson-parent-2.6.1.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.16/slf4j-parent-1.7.16.pom\n",
      "Downloaded https://repo1.maven.org/maven2/com/fasterxml/jackson/jackson-parent/2.6.1/jackson-parent-2.6.1.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-parent/40/commons-parent-40.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-parent/1.7.16/slf4j-parent-1.7.16.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/35/commons-parent-35.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-parent/35/commons-parent-35.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/34/commons-parent-34.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-parent/34/commons-parent-34.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/23/commons-parent-23.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/commons/commons-parent/23/commons-parent-23.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/commons/commons-parent/17/commons-parent-17.pom\n"
     ]
    }
   ],
   "source": [
    "import $file.sparksession\n",
    "import sparksession._\n",
    "import spark.implicits._\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._, func._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On `DataFrame`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create datasets from external data sources using different formats, e.g. Json, parquet, CSV, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val people: DataFrame = spark.read.json(\"data/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val temperature: DataFrame = spark.read.csv(\"data/economic-damage-from-natural-disasters.csv\")\n",
    "val peopleDFCsv = spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"data/economic-damage-from-natural-disasters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we created a `DataFrame`, not a `Dataset`. Dataframes are like datasets, i.e. programs to generate distributed data sets, but *dynamically typed*. This means that, at compile time, Scala only knows that a dataframe consists of `Row`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.collect\n",
    "peopleDFCsv.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, a `DataFrame` is defined as an alias of `Dataset`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val peopleDs: Dataset[Row] = people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the type of the information to be processed is there! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.schema\n",
    "people.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can convert a dataframe into a dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Esto es lo que tenemos que hacer \n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "\n",
    "case class Person(name: String, age: Long)\n",
    "\n",
    "val peopleDs: Dataset[Person] = people.as[Person]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDs.show\n",
    "people.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untyped transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` API includes a section on _untyped transformations_. These are transformations that are not defined over the Scala types but over the inner Spark SQL types (i.e. `StructType`s). More exactly, these could be named *dynamically typed transformations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformations are in close corresponde with their SQL counterparts: `SELECT`, `WHERE`, `GROUP BY`, `FROM`, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `select` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the equivalent to the `map` typed transformation is `select`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[String] = peopleDs.map(_.name)\n",
    "ds.collect\n",
    "ds.show\n",
    "ds.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df: DataFrame = \n",
    "    spark.read.json(\"data/people.json\").select($\"name\")\n",
    "df.collect\n",
    "df.show\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we lost the column label (`name`) in the case of the dataset transformation. This is not happening with `select`. Moreover, we have more control over the resulting schema: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDs.map(p => (p.name, p.age + 1, p.name.substring(0,3)))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.select($\"name\", $\"age\" + 1 as \"age\", $\"name\".substr(0,3) as \"prefix\")\n",
    "    .show\n",
    "\n",
    "//por defecto, el nombre de las columnas: la expresion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature.select($\"_c2\",$\"_c3\").show\n",
    "//renombrar las columnas\n",
    "val d:DataFrame = temperature.toDF(\"type\",\"code\",\"year\",\"money\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.select($\"type\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [org.apache.spark.sql.functions](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$) contains dozens of column operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that _untyped_, or more properly, _dynamically typed_, character means that the Scala compiler won't complain if we choose a non-existent column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy val df: DataFrame = spark.read.json(\"data/people.json\").select($\"nam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error will be shown at runtime: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the contrary, the error in the dataset transformation manifests at compile-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDs.map(_.nam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `filter` transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equivalent to the typed `filter` transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.filter($\"age\" > 2001)\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass a column function not denoting a boolean value, we won't even get a run-time exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df: DataFrame = \n",
    "    people.filter($\"name\" > 2001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `groupBy` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val students: DataFrame = spark.read.json(\"data/students.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.groupBy($\"degree\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.groupBy($\"degree\").mapGroup((key,value) => (key, value.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Join` transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already discussed joins, but we didn't mention that the resulting type of a join is a dataframe, not a dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Student(name: String, degree: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDs.join(students.as[Student], \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res31.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problems of `Dataset`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are nice because they are type safe, but, unfortunately, they are less efficient than data frames in several respects. This can be best shown by reading from parquet source files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a _columnar_ format, which means that it stores physically data around columns, allowing us to read only data from a particular column without reading the entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.write.mode(\"overwrite\").parquet(\"data/people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"data/people.parquet\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `ReadSchema` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a program that simply read the _name_ column of the people dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[String] = \n",
    "    spark.read.parquet(\"data/people.parquet\").as[Person]\n",
    "        .map(_.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which works as intended: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem, however: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the plan includes the directive `ReadSchema: struct<age:bigint,name:string>`, which generates a query to scan the full schema of the parquet file. But we just want to read the names! We can create an optimun program using dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df: DataFrame = \n",
    "    spark.read.parquet(\"data/people.parquet\").select($\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which works similarly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but more efficiently (note the the value of the `ReadSchema` directive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can empirically check that it actually works using the Spark UI. First, we create a parquet file with enough rows and several columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0, 1000000)\n",
    "    .select($\"id\" as \"_1\", lit(1) as \"_2\")\n",
    "    .write.mode(\"overwrite\").parquet(\"data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we read the second column using both datasets and dataframes, and check the Spark UI for the _Input Size_ field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test = spark.read.parquet(\"data/test\")\n",
    "test.as[Tuple2[Long, Int]].map(_._2).collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dataframes the input size is much lower since we only read the second column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.select($\"_2\").collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `PushedFilter` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following equivalent dataset and dataframe programs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds: Dataset[(Long, Int)] = \n",
    "    test.as[(Long, Int)]\n",
    "        .filter(_._1 >= 999995)\n",
    "\n",
    "val df: DataFrame = \n",
    "    test\n",
    "        .filter($\"_1\" >= 999995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionally, they are equivalent, but their performance differ significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect\n",
    "ds.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation of this difference lies in another optimization applied by the Spark SQL compiler: the so-called push-down filter optimization. In the previous `ReadSchema` optimization, we skipped certain columns of the dataset; now, we skip rows and read only the ones we are interested in (those that satisfy the predicate). We can check if the push-down filter optimization is actually applied by inspecting the query plan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain\n",
    "ds.explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `PartitionFilters` optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a test file with an additional column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0, 1000000)\n",
    "    .select($\"id\" as \"_1\", lit(1) as \"_2\", round(rand() * 10) mod lit(10) as \"_3\")\n",
    "    .write.mode(\"overwrite\").parquet(\"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test: DataFrame = spark.read.parquet(\"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that we want to read data with value `_3` equal to `9.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.filter($\"_3\" === lit(9.0)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pushed filter optimization is created, but it would be better if we could just read directly those rows with the exact value for the thrid column. We can achieve that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.write.mode(\"overwrite\").partitionBy(\"_3\").parquet(\"data/testP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the parquet file is splitted into ten partitions. Now, if we just want to process data with a particular key, Spark will generate an optimun query: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testP: DataFrame = spark.read.parquet(\"data/testP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testP.filter($\"_3\" === lit(9.0)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspet the Spark UI to check that we read less data in the last action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
