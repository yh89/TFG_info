{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defensa del trabajo de fin de grado de informática:\n",
    "### OPTIMIZACIÓN DE QUERIES DE BIG DATA CON SPARK EN AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo general: profundizar en la programación funcional y en el ecosistema de Scala, particularmente Spark, utilizando ejemplos de prueba sacados del dominio de los desastres naturales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, importamos las librerías que necesitamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $file.sparksession\n",
    "import sparksession._\n",
    "import spark.implicits._\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._, func._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsv(input: String ): DataFrame ={\n",
    "    spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"data/\" + input + \".csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Query sobre desastres naturales en Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query en Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object Model{\n",
    "    case class Earthquake(Entity: String,\n",
    "                        Code: String,\n",
    "                        Year: Int,\n",
    "                        Number: Int)\n",
    "}\n",
    "import Model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val  earthquake: Dataset[Earthquake] = readCsv(\"significant-earthquakes\").as[Earthquake]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = earthquake.filter(_.Year > 1700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencia entre Scala Collections y Dataset:  laziness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//dataset de prueba\n",
    "val lista = List(\n",
    "        Earthquake(\"Afghanistan\", \"AFG\",1601,0),\n",
    "        Earthquake(\"Afghanistan\", \"AFG\",1691,5),\n",
    "        Earthquake(\"Afghanistan\", \"AFG\",1701,2),\n",
    "        Earthquake(\"Spain\", \"ESP\",1702,0),\n",
    "        Earthquake(\"Spain\", \"ESP\",1690,5),\n",
    "        Earthquake(\"United States\", \"USA\",1501,3),\n",
    "        Earthquake(\"France\", \"FRA\",1901,6)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista.filter(_.Year > 1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query en Dataframe y la diferencia con Dataset: Pushed filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csv : DataFrame = readCsv(\"significant-earthquakes\").filter($\"Year\">1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val parquet: DataFrame = \n",
    "    spark.read.parquet(\"data/earthquakes.parquet\").filter($\"Year\">1700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráfica con Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.2`\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aux1 = csv.collect\n",
    "Bar(aux1.map(_.getAs[Int](2)).toSeq,\n",
    "    aux1.map(_.getAs[Int](3)).toSeq).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val temperatureByCountry : DataFrame = readCsv(\"GlobalLandTemperaturesByCountry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val temperatureByCountryYear:DataFrame = temperatureByCountry.withColumn(\"dt\", date_format(col(\"dt\"), \"yyyy\").cast(\"Int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val temperatureAVGTotal = temperatureByCountryYear\n",
    "                    .groupBy($\"dt\")\n",
    "                    .avg(\"AverageTemperature\")\n",
    "                    .orderBy($\"dt\")\n",
    "                    .collect\n",
    "Scatter(\n",
    "    temperatureAVGTotal.map(_.getInt(0)).toSeq,\n",
    "    temperatureAVGTotal.map(_.getAs[Double](1)).toSeq\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráfica en la plataforma Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas con AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones y trabajo futuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En este trabajo se ha profundizado en la programación funcional con las librerías de Scala:\n",
    "Principalmente con Spark y se ha analizado las optimizaciones que ofrece Spark, también se ha utilizado la librería plotly para hacer gráficas. \n",
    "- Se ha desarrollado el trabajo en varios entornos: Jupyter Notebook, Sbt, Databricks y AWS.\n",
    "- Y se ha hecho un pequeño estudio sobre las bases de desastres naturales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre el trabajo futuro:\n",
    "- Se puede seguir profundizando en la librería Spark con una base de datos más grandes,\n",
    "- Aprender a diseñar un DSL embebido.\n",
    "- Conocer y aprender sobre los servicios que da en las plataformas como AWS, Azure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
