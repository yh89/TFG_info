{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defensa del trabajo de fin de grado de informática:\n",
    "### OPTIMIZACIÓN DE QUERIES DE BIG DATA CON SPARK EN AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La programación funcional es un paradigma de programación declarativa que se basa en las funciones matemáticas que nos permite tener códigos más limpios y estructurados, por tanto más modulares y todas las ventajas que llevan a ser modular. Como la disminución de la complejidad de los algoritmos, el ahorro en el tiempo de programación porque promueve la reusabilidad del código, y facilita la prueba y el mantenimiento.\n",
    " \n",
    "Por el otro lado, con los avances tecnológicos en el campo de hardware, tenemos cada vez más núcleos para procesar, por tanto el software tiene que gestionar eficientemente los núcleos para sacar el máximo rendimiento. Pero con la programación imperativa, la que tenía el mayor mercado, surgen los problemas con la memoria compartida, lo que ralentizan los procesos. Pero con la programación funcional no hay ese problema por el uso de funciones puras que no producen efectos colaterales. Por eso la programación funcional está teniendo grandes crecimientos en el ámbito industrial en los últimos años. Por ejemplo la infraestructura de Whatsapp y la de Twitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los objetivos de este trabajo consiste en profundizar en la programación funcional, sobre todo en DSLs. \n",
    "También es un objetivo profundizar en el ecosistema de librerías de Scala: la librería Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar importamos las librería que necesitamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $file.sparksession\n",
    "import sparksession._\n",
    "import spark.implicits._\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{functions => func, _}\n",
    "import org.apache.spark.sql.types._, func._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsv(input: String ): DataFrame ={\n",
    "    spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(\"data/\" + input + \".csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La evaluación laziness de Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object Model{\n",
    "    case class Disaster(Entity: String,\n",
    "                   Year: Int,\n",
    "                   Number: Int)\n",
    "}\n",
    "\n",
    "//dataset de prueba\n",
    "val lista = List(Model.Disaster(\"All natural disasters\", 2000, 50000),\n",
    "        Model.Disaster(\"All natural disasters\", 2001, 40000),\n",
    "        Model.Disaster(\"Flood\", 2000, 20000),\n",
    "        Model.Disaster(\"Hurracan\", 2000, 20000),\n",
    "        Model.Disaster(\"fire\", 2000, 10000),\n",
    "        Model.Disaster(\"Flood\", 2001, 20000),\n",
    "        Model.Disaster(\"Starve\", 2001, 20000))\n",
    "val listDS = lista.toDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista.filter(_.Number > 20000)\n",
    "//los que tenga el elemento en la columna number\n",
    "//mayor que 20000\n",
    "val dsFilter = listDS.filter($\"Number\" > 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsFilter.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushed filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds : Dataset[String] = readCsv(\"GlobalLandTemperaturesByCountry\").map(_.getAs[String](\"Country\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csv : DataFrame = readCsv(\"GlobalLandTemperaturesByCountry\").filter($\"Country\" === \"Denmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val parquet: DataFrame = \n",
    "    spark.read.parquet(\"data/temperatureByCountry.parquet\").filter($\"Country\" === \"Denmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.filter(_ == \"Denmark\").explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PatitionFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//filter sin partition\n",
    "val sin: DataFrame = \n",
    "    spark.read.parquet(\"data/temperatureByCity.parquet\")\n",
    "            .filter($\"Country\" === \"Denmark\" && $\"AverageTemperature\" >10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//filter con partition\n",
    "val con = spark.read.parquet(\"data/country.parquet\")\n",
    "            .filter($\"Country\" === \"Denmark\" && $\"AverageTemperature\" >10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfica con Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro de los objetivo es reproducir unos resultados sobre unas bases de datos de desastres naturales, para ello hace falta hacer gráficas. En este caso he utilizado la librería plotly para conseguirlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.2`\n",
    "\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.Almond._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val disasterNumber = readCsv(\"number-of-natural-disaster-events\").drop($\"Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aux = disasterNumber.filter($\"Entity\" ===\"All natural disasters\").orderBy($\"Year\").select($\"Year\",$\"Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aux1 = aux.collect\n",
    "Bar(aux1.map(_.getInt(0)).toSeq,\n",
    "    aux1.map(_.getInt(1)).toSeq).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfica en Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enseñar los pasos en EMR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
